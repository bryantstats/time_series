
---
title: "ARIMA Models (1)"
author: <font size="5"> Son Nguyen </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>`
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #F9C389;
  font-size: 17px;
  font-weight: 300;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.inverse {
  background-color: #696767;
  border-top: 80px solid #696767;
  text-shadow: none;
  background-image: url(https://github.com/goodekat/presentations/blob/master/2019-isugg-gganimate-spooky/figures/spider.png?raw=true);
	background-position: 50% 75%;
  background-size: 150px;
}

.your-turn{
  background-color: #8C7E95;
  border-top: 80px solid #F9C389;
  text-shadow: none;
  background-image: url(https://github.com/goodekat/presentations/blob/master/2019-isugg-gganimate-spooky/figures/spider.png?raw=true);
	background-position: 95% 90%;
  background-size: 75px;
}

.title-slide {
  background-color: #F9C389;
  border-top: 80px solid #F9C389;
  background-image: none;
}

.title-slide > h1  {
  color: #111111;
  font-size: 40px;
  text-shadow: none;
  font-weight: 400;
  text-align: left;
  margin-left: 15px;
  padding-top: 80px;
}
.title-slide > h2  {
  margin-top: -25px;
  padding-bottom: -20px;
  color: #111111;
  text-shadow: none;
  font-weight: 300;
  font-size: 35px;
  text-align: left;
  margin-left: 15px;
}
.title-slide > h3  {
  color: #111111;
  text-shadow: none;
  font-weight: 300;
  font-size: 25px;
  text-align: left;
  margin-left: 15px;
  margin-bottom: -30px;
}

</style>

```{css, echo=FALSE}
.left-code {
  color: #777;
  width: 48%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 51%;
  float: right;
  padding-left: 1%;
}
```

```{r setup, include = FALSE}

# R markdown options
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 10,
                      fig.height = 5,
                      fig.align = "center", 
                      message = FALSE,
                      warning = FALSE)

# Load packages
library(tidyverse)
library(forecast)
```

# ARIMA Models

The ARIMA models is a combination of three models:

(1) The Autoregressive Models  (AR)

(2) The Integrated Models (I)

(3) The Moving Average Models (MA)

We already see the AR models last week.  We will next look at the MA and the ARMA models. 

---
class: inverse, center, middle

# Moving Average Models

---
# MA(1)

The first order moving average model or simple moving average model, or MA(1) is defined as follows. 

$$Y_t = \mu  + \theta \epsilon_{t-1} + \epsilon_t$$

where $\epsilon \sim (0, \sigma^2)$

The MA(1) has three parameters: $\mu$, $\theta$ and $\sigma^2$

---
# MA Smoothing vs. MA Models

- MA models should not be confused with the MA smoothing

- A MA model is used for forecasting future values

- MA smoothing is used for estimating the trend-cycle of past values.

- They are two different concepts. 

---
# Example 

Let generate the MA(1) with the mean $\mu = 7$, the slope $\theta = .6$ and the variance of the noise $\sigma^2 =1$. Or, 

$$y_t = 7 + .6 \epsilon_{t-1} + \epsilon,$$
where $\epsilon \sim (0, 1)$

---
# Example 

Similar to the AR models, We use the `arima.sim` to simulate the series. 


```{r}
mu = 7
theta = .6
yt <- arima.sim(list(order=c(0,0,1), ma=c(theta)), n=100)
yt <- yt + mu
```

---
# Example 

```{r}
plot(yt)
```


---
# Theoretical Properties

$$Y_t = \mu  + \theta \epsilon_{t-1} + \epsilon_t$$

where $\epsilon \sim (0, \sigma^2)$

- The mean of the series

$$E(y_t) =  \mu$$

- The Variance of the series

$$V(y_t) =  \sigma^2(1+\theta^2)$$
- The autocorrelation function (ACF) at lag 1 is $ACF(1) = p_1 =  \frac{\theta}{1+\theta^2}$

- The autocorrelation function (ACF) at lag k is $ACF(k) = p_k =  0$ for $k>1$

- Notice that the ACF at lag 0 is always 1 for any time series. 

- It can also be shown the the MA(1) series is stationary. 


---
# Example

For the previously simulated series,

$$y_t = 7 + .6 \epsilon_{t-1} + \epsilon$$

We have $$E(y_t) = 7$$, $$V(y_t) = (1+\theta^2) = 1+.6^2 = 1.36$$ and $$p_1 = \frac{.6}{1+.6^2} = 0.4411765$$

---
# Example

Let verify some of the theoretical properties of the previously simulated series. 

```{r}
set.seed(1234)
yt <- arima.sim(list(order=c(0,0,1), ma=c(theta)), n=100000)
yt <- yt + mu

# calculate the mean
mean(yt)

# calculate the variance 
var(yt)
```

We observe that the sample mean and variance are very close to their theoretical values. 

---
# Example

Let verify some of the theoretical properties of the previously simulated series. 

```{r}
acf(yt)$acf[2]
```

We observe that the sample values the ACF at lag 1 is very close its theoretical value. 

---
class: inverse, center, middle
# AR(1) vs. MA(1)

---
# AR(1) vs. MA(1)

- Both AR(1) and MA(1) are stationary so it is not easy to tell the different looking at the series plots.

- Let compare a simulated AR(1) and MA(1) side by side

```{r}
y_ma <- arima.sim(list(order=c(0,0,1), ma=c(.1)), n=100)
y_ar <- arima.sim(list(order=c(1,0,0), ar=c(.1)), n=100)

par(mfrow = c(1, 2))
plot(y_ma)
plot(y_ar)
```

---
# AR(1) vs. MA(1)

- Another comparison side by side

```{r}
y_ma <- arima.sim(list(order=c(0,0,1), ma=c(.5)), n=100)
y_ar <- arima.sim(list(order=c(1,0,0), ar=c(.5)), n=100)

par(mfrow = c(1, 2))
plot(y_ma)
plot(y_ar)
```


---
# AR(1) vs. MA(1)

- Another comparison side by side

```{r}
y_ma <- arima.sim(list(order=c(0,0,1), ma=c(-.5)), n=100)
y_ar <- arima.sim(list(order=c(1,0,0), ar=c(-.5)), n=100)

par(mfrow = c(1, 2))
plot(y_ma)
plot(y_ar)
```

---
# AR(1) vs. MA(1)

Although it is not easy to distinguish the plot of the two series, we can look at their ACF to see the difference. Notice that

- MA(1) has non-zero autocorrelation at lag 1 only

- AR(1) has non-zero autocorrelation at many lags

---
# AR(1) vs. MA(1)

```{r}
y_ma <- arima.sim(list(order=c(0,0,1), ma=c(.8)), n=1000)
y_ar <- arima.sim(list(order=c(1,0,0), ar=c(.8)), n=1000)

par(mfrow = c(1, 2))
acf(y_ma)
acf(y_ar)
```

- Notice that MA(1) has non-zero autocorrelation at lag 1 only.  AR(1) has non-zero autocorrelation at many lags before dying out to zeros.

---
# AR(1) vs. MA(1)

```{r}
y_ma <- arima.sim(list(order=c(0,0,1), ma=c(-.8)), n=1000)
y_ar <- arima.sim(list(order=c(1,0,0), ar=c(-.8)), n=1000)

par(mfrow = c(1, 2))
acf(y_ma)
acf(y_ar)
```

- Notice that MA(1) has non-zero autocorrelation at lag 1 only.  AR(1) has non-zero autocorrelation at many lags before dying out to zeros.


---
class: inverse, center, middle
# MA(2)

---
# MA(2)

Similar to MA(1), MA(2) is defined by the below equation

$$Y_t = \mu  + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \epsilon_t$$

where $\epsilon \sim (0, \sigma^2)$

---
# Example 

Let simulate the below MA(2) series

$$Y_t = 9  + .5 \epsilon_{t-1} + .3 \epsilon_{t-2} + \epsilon_t$$

where $\epsilon \sim (0, 1)$


---
# Example 

We use the same `arima.sim` function. 

```{r}
set.seed(1234)

mu = 9
theta1 = .5
theta2 = .3

yt <- arima.sim(list(order=c(0,0,2), ma=c(theta1, theta2)), n=100)
yt <- yt + mu
```

---
# Example 

We use the same `arima.sim` function. 

```{r}
plot(yt)
```

---
# Theoretical Properties

$$Y_t = \mu  + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \epsilon_t$$

where $\epsilon \sim (0, \sigma^2)$

- The mean of the series

$$E(y_t) =  \mu$$

- The Variance of the series

$$V(y_t) =  \sigma^2(1+\theta^2_1 + \theta^2_2)$$

- The autocorrelation function (ACF) at lag 1 is $ACF(1) = p_1 =  \frac{\theta_1 + \theta_1\theta_2}{1+\theta^2_1 + \theta^2_2}$

- The autocorrelation function (ACF) at lag 2 is $ACF(2) = p_2 =  \frac{\theta_2}{1+\theta^2_1 + \theta^2_2}$

- The autocorrelation function (ACF) at lag k is $ACF(k) = p_k =  0$ for $k>2$

- It can also be shown the the MA(2) series is stationary. 

---
# Example

- Calculate the mean, variance and the ACF at lag 0, 1, and 2 of the previous series. 

$$Y_t = 9  + .5 \epsilon_{t-1} + .3 \epsilon_{t-2} + \epsilon_t$$

where $\epsilon \sim (0, 1)$

- Verify the these values by simulation. 

---
class: inverse, center, middle
# MA(q)

---
# MA(q)

We extend the MA family to MA(q) series as defined below. 

$$Y_t = \mu  + \theta_1 \epsilon_{t-1} +...+\theta_q \epsilon_{t-q} + \epsilon_t$$

where $\epsilon \sim (0, \sigma^2)$

---
# Simulation

```{r}
# MA(3)
mu = 10
theta1 = .1
theta2 = .2
theta3 = .3
  
yt <- arima.sim(list(order=c(0,0,3), ma=c(theta1, theta2, theta3)), n=100)

yt <- yt + mu
```


---
# Simulation

```{r}
# MA(3)
plot(yt)
```


---
# Simulation

```{r}
# MA(4)
mu = 10
theta1 = .1
theta2 = .2
theta3 = .3
theta4 = .5
  
yt <- arima.sim(list(order=c(0,0,4), ma=c(theta1, theta2, theta3, theta4)), n=100)

yt <- yt + mu
```

---
# Theoretical Properties

$$Y_t = \mu  + \theta_1 \epsilon_{t-1} +...+\theta_q \epsilon_{t-q} + \epsilon_t$$

where $\epsilon ~ (0, \sigma^2)$

- The mean of the series

$$E(y_t) =  \mu$$

- The Variance of the series

$$V(y_t) =  \sigma^2(1+\theta^2_1 + \theta^2_2 + ... + \theta_q^2)$$

- The autocorrelation function (ACF) at lag k for $k>q$ are all zeros. 

- The fact that the ACF are all zeroes after lag $q$ is useful when we want to fit an MA model to a dataset. 



---
# Simulation

```{r}
# MA(4)
set.seed(12)
yt <- arima.sim(list(order=c(0,0,4), ma=c(.1, .2, .5, .9)), n=100)
acf(yt)
```

- ACF of an MA(4). Notice that the latest non-zero lag is 4. 


---
# Simulation

```{r}
# MA(2)
set.seed(122222)
yt <- arima.sim(list(order=c(0,0,2), ma=c(.1, .9)), n=100)
acf(yt)
```

- ACF of an MA(2). Notice that the latest non-zero lag is 2. 



---
# Simulation

```{r}
# MA(2)
set.seed(1222)
yt <- arima.sim(list(order=c(0,0,3), ma=c(.1, .9, .7)), n=100)
acf(yt)
```

- ACF of an MA(3). Notice that the latest non-zero lag is 3. 



---
# Modeling with MA

Based on the properties of the MA(q) model discussed above, we may consider to fit an MA model to a dataset if 

- The data series is/looks stationary

- The ACF of the first few lags are non-zeros and then die out to zeroes after that

Also from the ACF, we can decide the order of the MA as the latest non-zero lag before the ACF dies out to zeroes. 

---
# Example

Consider The Lake Erie data. The series is n = 40 consecutive annual measurements of the level of Lake Erie in October.

[Dataset](eriedata.csv)

```{r}
# read the dataset
df = read.csv('eriedata.csv')

# create the time series object from the dataset

y = ts(df$level)
```


---
# Example


```{r}
plot(y)
```

- Although there seems to be a slight upward trend, we could ignore this and assume that the series has no trend and could be stationary.

---
# Example


```{r}
acf(y)
```

- The ACF dying out to zeros suggests that we could assume this series is stationary. 

---
# Example 

- By looking at the plot of the series and its ACF, we think that it may be reasonable to say that the series is stationary. Thus, it is reasonable to fit the MA model to this dataset. 

- The ACF also suggests the order of the MA model. Since the latest non-zero lag before the ACF dies out to zeros is 3 (ACF at lag 0, lag 1, lag 2, and lag 3 are out of the blue strip or non-zero. ACF at lag 4, 5, 6 and so on are in the blue strip or 0), we will fit the MA(3) model to this series.

---
# Fit the Model 

We will use the `arima` function to fit the model to the series data

```{r}
# fit an MA(3) model to the data
y_ma = arima(y, order = c(0,0,3))
y_ma
```

---
# Plot 

We plot the fitted values and the original values of the series

```{r}
plot(y)
lines(y-y_ma$residuals, col = "red")
```

---
# Forecast

We then can make forecast using the model. 

```{r}
y_forecast = forecast(y_ma)
plot(y_forecast)
```

---
class: inverse, center, middle
# ARMA Models

---
# ARMA Models

- We will now combine the AR and MA model together in an ARMA model

- ARMA(p,q) is a combination of AR(p) and MA(q)

- ARMA(p, q) is a stationary process and can be used to model a stationary series


---
# ARMA Models

The ARMA(p, q) model is defined as

$$Y_t = \mu + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \ldots + \phi_p Y_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q}$$
where $\epsilon \sim (0, \sigma^2)$

- $\phi_1, \phi_2, ...,$ are the autoregressive parameters representing the relationship between the current value of the series and its past values up to lag $p$

- $\theta_1, \theta_2, ...,$ are the moving average parameters representing the relationship between the current error term and its past values up to lag $q$.


---
# Simulation

We use the `arima_sim` to simulate data from this model. 

```{r}
# Simulate data from the ARMA(1,2)
set.seed(2024)
y = arima.sim(list(order=c(1,0,2), ar=c(.1), ma = c(.1, .4)), n=100)
```

---
# Simulation

We use the `arima_sim` to simulate data from this model. 

```{r}
# Simulate ARMA(1,2)
plot(y)
```

---
# Fitting ARMA Models

- ARMA can be considered to fit a stationary series.

- We know that we can use the ACF to estimate/suggest the order of the MA part (parameter $q$)

- What about the order of the AR part (parameter $p$)?

- We will use the Partial ACF to suggest $p$!

---
class: inverse, middle, center
# Partial ACF


---
# Partial Autocorrelation

If $X$, $Y$ and $Z$ are random variables then the partial autocorrelation between $X$ and $Y$ given $Z$ is the correlation between $X$ and $Y$ with the linear effect of $Z$ removed from both $X$ and $Y$. 

- Regress $X$ on $Z$ to obtain $\hat{X}$, the linear effect of $Z$ in $X$, 
- $X -\hat{X}$ is $X$ with the linear effect of $Z$ removed 
- Regress $Y$ on $Z$ to obtain $\hat{Y}$, the linear effect of $Z$ in $Y$, 
- $Y -\hat{Y}$ is $Y$ with the linear effect of $Z$ removed 

$$
p_{XY|Z} = corr(X-\hat{X}, Y- \hat{Y})
$$

---
# PACF

We define the Partial ACF or PACF of a time series $y_t$ as follows. 

$$p_k =  Corr(Y_t, Y_{t-k}|Y_{t-1}, Y_{t-2},...,Y_{t-k+1})$$
Thus, 

- $p_2$ be the partial autocorrelation between $y_t$ and $y_{t-2}$ given $y_{t-1}$ (removing the effect of $y_t-1$)

- $p_3$ be the partial autocorrelation between $y_t$ and $y_{t-3}$ given $y_{t-1}, y_{t-2}$ (removing the effect of $y_{t-1}, y_{t-2}$)

- And so on

---
# PACF of AR(p) models

The PACF of the AR(p) models has this following nice property

$$p_{k} =
\begin{cases}
\phi_k & \text{for } k = 1, 2, \ldots, p \\
0 & \text{for } k > p
\end{cases}$$

where $\phi_1, \phi_2, ...,$ are the autoregressive parameters. 

- This property helps us determine the order of the AR in the ARMA based on the PACF of the time series. 

---
# PACF of AR(p) models

For example, consider an AR(2) model

$$y_t = \beta_0 + \beta_1y_{t-1}+ \beta_2y_{t-2} + \epsilon$$
- Then

$$PACF(3) =  PACF(4) = ...= 0$$

Thus, if the PACF of a stationary series has the first two values are non-zeros (two first spikes) then zeros afterward, we could consider fitting an AR(2) or ARMA(2, q) to the dataset. 

---
# Example

Let simulate some ARMA and verify this property. We will simulate data from the ARMA(3,1) model. 


```{r}
set.seed(2024)
yt <- arima.sim(list(order=c(3,0,1), ar=c(.1, .65, .2), ma = c(.1)), n=10000)
b0 = 10
yt <- yt + b0
```

Since the AR component has order 3, we would expect the PACF are non-zeros in the first three values then zeros afterward. 

---
# Example

Plot the PACF using the `pacf` function

```{r}
pacf(yt)
```

We notice that the PACF has three spikes (non-zero values) from the first three values as expected.


---
# Fitting ARMA to data

We will use the Lake Erie data again. The series is n = 40 consecutive annual measurements of the level of Lake Erie in October.

[Dataset](eriedata.csv)

```{r}
# read the dataset
df = read.csv('eriedata.csv')

# create the time series object from the dataset

y = ts(df$level)
```


---
# Fitting ARMA to data

Previously, we fit the MA(3) model to this series.  Now. We will add an AR component and fit the data with the ARMA model. To decide the order of the AR component, let plot the PACF


```{r}
pacf(y)
```

- The PACF is non-zero at its first value then dies out to zeroes afterward.  This suggests that we could use the first order ( $p=1$ ) AR component, or AR(1) to fit the data. 

---
# Fitting

- Combining AR(1) with MA(3) decided previously, we have the ARMA(1, 3) to be a suggested model to fit to this series. 


```{r}
# fit an ARMA(1, 3) model to the data
y_arma = arima(y, order = c(1,0,3))
```

---
# Plot 

We plot the fitted values and the original values of the series

```{r}
plot(y)
lines(y-y_arma$residuals, col = "blue")
```


---
# Forecast

We then can make forecast using the model. 

```{r}
y_forecast = forecast(y_arma)
plot(y_forecast)
```

---
# Summary

When deciding the orders of the ARMA we look at the ACF (for the order of the MA component) and PACF (for the order of the AR component) of the series. Specifically,

- If the latest non-zero lag of the ACF is k, then it suggest to use MA(k)

- If the latest non-zero of lag the PACF is h, then it suggest to use AR(h)

Thus, together the ACF and PACF suggest to use ARMA(h, k) to fit the data. 
